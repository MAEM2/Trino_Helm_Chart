alluxio:
  fullnameOverride: alluxio

  image: alluxio/alluxio
  imageTag: 2.7.2
  imagePullPolicy: IfNotPresent

  tieredstore:
    levels:
      - level: 0
        alias: MEM
        mediumtype: MEM
        path: /dev/shm
        type: emptyDir
        quota: 1G
        high: 0.95
        low: 0.7
  shortCircuit:
    enabled: true
    # The policy for short circuit can be "local" or "uuid",
    # local means the cache directory is in the same mount namespace,
    # uuid means interact with domain socket
    policy: uuid
    # volumeType controls the type of shortCircuit volume.
    # It can be "persistentVolumeClaim" or "hostPath"
    volumeType: persistentVolumeClaim
    size: 1Mi
    # Attributes to use if the domain socket volume is PVC
    pvcName: alluxio-worker-domain-socket
    accessModes:
      - ReadWriteOnce
    storageClass: standard
    # Attributes to use if the domain socket volume is hostPath
    hostPath: "/tmp/alluxio-domain" # The hostPath directory to use
  worker:
    enabled: true
    hostname: alluxio-worker-0
    count: 1
    user: 0
    env:
    # Extra environment variables for the worker pod
    # Example:
    # JAVA_HOME: /opt/java
    args:
      - worker-only
      - --no-format
    # Properties for the worker component
    properties:
    resources:
      limits:
        cpu: "4"
        memory: "4G"
      requests:
        cpu: "1"
        memory: "2G"
    ports:
      rpc: 29999
      web: 30000
    # hostPID requires escalated privileges
    hostPID: false
    hostNetwork: false
    # dnsPolicy will be ClusterFirstWithHostNet if hostNetwork: true
    # and ClusterFirst if hostNetwork: false
    # You can specify dnsPolicy here to override this inference
    # dnsPolicy: ClusterFirst
    # JVM options specific to the worker container
    jvmOptions:
    nodeSelector: {}
    readinessProbe:
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1
    livenessProbe:
      initialDelaySeconds: 15
      periodSeconds: 30
      timeoutSeconds: 5
      failureThreshold: 2
    # If you are using Kubernetes 1.18+ or have the feature gate
    # for it enabled, use startupProbe to prevent the livenessProbe
    # from running until the startupProbe has succeeded
    # startupProbe:
    #   initialDelaySeconds: 15
    #   periodSeconds: 30
    #   timeoutSeconds: 5
    #   failureThreshold: 2
    tolerations: []
    podAnnotations: {}
    # The ServiceAccount provided here will have precedence over
    # the global `serviceAccount`
    serviceAccount:
    # Setting fuseEnabled to true will embed Fuse in worker process. The worker pods will
    # launch the Alluxio workers using privileged containers with `SYS_ADMIN` capability.
    # Be sure to give root access to the pod by setting the global user/group/fsGroup
    # values to `0` to turn on Fuse in worker.
    fuseEnabled: false

  jobWorker:
    args:
      - job-worker
    # Properties for the jobWorker component
    properties:
    resources:
      limits:
        cpu: "4"
        memory: "4G"
      requests:
        cpu: "1"
        memory: "1G"
    ports:
      rpc: 30001
      data: 30002
      web: 30003
    # JVM options specific to the jobWorker container
    jvmOptions:
    readinessProbe:
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1
    livenessProbe:
      initialDelaySeconds: 15
      periodSeconds: 30
      timeoutSeconds: 5
      failureThreshold: 2

server:
  workers: 4
  node:
    environment: production
    dataDir: /data/trino
    pluginDir: /data/trino/plugin
  log:
    trino:
      level: INFO
  config:
    path: /etc/trino
    pathCatalog: /etc/trino/catalog
    http:
      port: 8080
    https:
      enabled: true
      port: 8443
    query:
      maxMemory: "20GB"
      maxTotalMemory: "32GB"
      maxMemoryPerNode: "6GB"
      maxTotalMemoryPerNode: "10GB"
  jvm:
    maxHeapSize: "24G"
    gcMethod:
      type: "UseG1GC"
      g1:
        heapRegionSize: "32M"
  jmx:
    registryPort: 9080
    serverPort: 9081

image:
  repository: trinodb/trino
  tag: 356
  pullPolicy: IfNotPresent
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000


service:
  type: ClusterIP
  externalType: NodePort

resources:
  coordinator:
    limits:
      cpu: 3000m
      memory: 30000Mi
    requests:
      cpu: 2000m
      memory: 24000Mi
  worker:
    limits:
      cpu: 2000m
      memory: 30000Mi
    requests:
      cpu: 1000m
      memory: 28000Mi

nodeSelector: {}

tolerations: []

affinity: {}

hive:
  metastore:
    uri: thrift://metastore-headless:9083
  s3:
    endpoint: https://host:port
    ssl:
      enabled: true
    max-connections: 100
storagegrid:
  secret: dma-dev-storagegrid-access-key
  accessKey:
  secretKey:
passwordfile:
  secret: trino-passworddb
  mountpath: /usr/lib/trino/etc
  filepath: /usr/lib/trino/etc/password.db
keystore:
  secret: trino-keystore
  mountpath: /usr/lib/trino/etc/keystore
  filepath: /usr/lib/trino/etc/keystore/keystore.jks
  key: key
exporters:
  jmx:
    enabled: true
    image:
      repository: sscaling/jmx-prometheus-exporter
      tag: 0.3.0
      pullPolicy: IfNotPresent
    port: 9999
    env: {}
    resources:
      coordinator:
        limits:
          cpu: 3000m
          memory: 10000Mi
        requests:
          cpu: 1000m
          memory: 8000Mi
      worker:
        limits:
          cpu: 1000m
          memory: 3000Mi
        requests:
          cpu: 800m
          memory: 2000Mi

    config:
      jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:9080/jmxrmi
      lowercaseOutputName: true
      lowercaseOutputLabelNames: true

      whitelistObjectNames:
      - 'trino.execution:*'
      - 'trino.execution.executor:*'
      - 'trino.execution.resourcegroups:*'
      - 'trino.failuredetector:*'
      - 'trino.memory:*'
      - 'trino.metadata:*'
      - 'trino.security:*'
      - 'trino.sql.gen:*'
      - 'trino.sql.planner.iterative:*'
      - 'trino.sql.planner.optimizations:*'
      rules:
        # Trino execution stats {{{
        - pattern: 'trino.execution<name=ClusterSizeMonitor><>RequiredWorkers'
          name: 'trino_cluster_size_monitor_required_workers'
          help: 'Number of workers required in cluster'
          type: GAUGE

        # Query counts {{{
        - pattern: 'trino.execution<name=QueryManager><>(Abandoned|Canceled|Completed|Failed|Started|Submitted)Queries.TotalCount'
          name: 'trino_query_manager_queries_total'
          help: 'Total number of queries'
          type: COUNTER
          labels:
            state: '$1'
        - pattern: 'trino.execution<name=QueryManager><>(Queued|Running)Queries'
          name: 'trino_query_manager_queries'
          help: 'Number of queries currently in the labelled state'
          type: GAUGE
          labels:
            state: '$1'
        # }}}

        - pattern: 'trino.execution<name=QueryManager><>ConsumedCpuTimeSecs.TotalCount'
          name: 'trino_query_manager_consumed_cpu_time_seconds_total'
          help: 'Total CPU time consumed across all queries in seconds'
          type: COUNTER
        - pattern: 'trino.execution<name=QueryManager><>ConsumedInputBytes.TotalCount'
          name: 'trino_query_manager_consumed_input_bytes_total'
          help: 'Total input bytes consumed across all queries in bytes'
          type: COUNTER
        - pattern: 'trino.execution<name=QueryManager><>ConsumedInputRows.TotalCount'
          name: 'trino_query_manager_consumed_input_rows_total'
          help: 'Total number of input rows consumed across all queries'
          type: COUNTER

        # CPU and Wall input byte rate histograms/summaries {{{
        - pattern: 'trino.execution<name=QueryManager><>(CpuInputByte|WallInputBytes)Rate.AllTime.(Avg|Min|Max)'
          name: 'trino_query_manager_$1_rate_$2'
          help: '$2 rate $1'
          type: GAUGE
        - pattern: 'trino.execution<name=QueryManager><>(CpuInputByte|WallInputBytes)Rate.AllTime.P(\d+)'
          name: 'trino_query_manager_$1_rate'
          help: '$1 rate for the labelled quantile'
          type: GAUGE
          labels:
            quantile: '0.$2'
        - pattern: 'trino.execution<name=QueryManager><>(CpuInputByte|WallInputBytes)Rate.AllTime.Count'
          name: 'trino_query_manager_$1_rate_count'
          help: '$1 rate count'
          type: COUNTER
        # }}}

        # Execution and queued time histograms/summaries {{{
        - pattern: 'trino.execution<name=QueryManager><>(Execution|Queued)Time.AllTime.(Avg|Min|Max)'
          valueFactor: 0.001
          name: 'trino_query_manager_$1_seconds_$2'
          help: '$2 $1 time in seconds'
          type: GAUGE
        - pattern: 'trino.execution<name=QueryManager><>(Execution|Queued)Time.AllTime.P(\d+)'
          valueFactor: 0.001
          name: 'trino_query_manager_$1_seconds'
          help: '$1 time in seconds for the labelled quantile'
          type: GAUGE
          labels:
            quantile: '0.$2'
        - pattern: 'trino.execution<name=QueryManager><>(Execution|Queued)Time.AllTime.Count'
          valueFactor: 0.001
          name: 'trino_query_manager_$1_seconds_count'
          help: '$1 time count'
          type: COUNTER
        # }}}

        - pattern: 'trino.execution<name=QueryManager><>(External|InsufficientResources|Internal|UserError)Failures.TotalCount'
          name: 'trino_query_manager_failures_total'
          help: 'Total number of failures with the labelled type'
          type: COUNTER
          labels:
            type: '$1'
        # }}}

        # Trino executor stats {{{
        # MLFQ stats {{{
        - pattern: 'trino.execution.executor<name=MultilevelSplitQueue><>Level(\d)Time'
          valueFactor: 0.000000001
          name: 'trino_multilevel_split_queue_level_time_seconds_total'
          help: 'Total time spent in the labelled level of the MLFQ in seconds'
          type: COUNTER
          labels:
            level: '$1'
        - pattern: 'trino.execution.executor<name=MultilevelSplitQueue><>SelectedCountLevel(\d).TotalCount'
          name: 'trino_multilevel_split_queue_level_selected_total'
          help: 'Total number of times the labelled level was selected'
          type: COUNTER
          labels:
            level: '$1'
        # }}}

        # BlockedQuantaWallTime, SplitQueuedTime, SplitWallTime and UnblockedQuantaWallTime histograms {{{
        - pattern: 'trino.execution.executor<name=TaskExecutor><>(BlockedQuantaWallTime|SplitQueuedTime|SplitWallTime|UnblockedQuantaWallTime).AllTime.(Avg|Min|Max)'
          valueFactor: 0.000001
          name: 'trino_task_executor_$1_seconds_$2'
          help: '$2 $1 in seconds'
          type: GAUGE
        - pattern: 'trino.execution.executor<name=TaskExecutor><>(BlockedQuantaWallTime|SplitQueuedTime|SplitWallTime|UnblockedQuantaWallTime).AllTime.P(\d+)'
          valueFactor: 0.000001
          name: 'trino_task_executor_$1_seconds'
          help: '$1 in seconds for the labelled quantile'
          type: GAUGE
          labels:
            quantile: '0.$2'
        - pattern: 'trino.execution.executor<name=TaskExecutor><>(BlockedQuantaWallTime|SplitQueuedTime|SplitWallTime|UnblockedQuantaWallTime).AllTime.Count'
          valueFactor: 0.000001
          name: 'trino_task_executor_$1_seconds_count'
          help: '$1 in seconds count'
          type: COUNTER
        # }}}

        - pattern: 'trino.execution.executor<name=TaskExecutor><>(Blocked|Intermediate|Running|Waiting)Splits'
          name: 'trino_task_executor_splits'
          help: 'Number of splits in the labelled state'
          type: GAUGE
          labels:
            state: '$1'
        - pattern: 'trino.execution.executor<name=TaskExecutor><>TotalSplits'
          name: 'trino_task_executor_splits_total'
          help: 'Total number of splits'
          type: COUNTER

        # Intermediate and Leaf splits CPU, scheduled, wait and wall times histograms {{{
        - pattern: 'trino.execution.executor<name=TaskExecutor><>(IntermediateSplitCpu|IntermediateSplitScheduled|IntermediateSplitWait|IntermediateSplitWall|LeafSplitCpu|LeafSplitScheduled|LeafSplitWait|LeafSplitWall)Time.(Avg|Min|Max)'
          valueFactor: 0.000001
          name: 'trino_task_executor_$1_seconds_$2'
          help: '$2 $1 in seconds'
          type: GAUGE
        - pattern: 'trino.execution.executor<name=TaskExecutor><>(IntermediateSplitCpu|IntermediateSplitScheduled|IntermediateSplitWait|IntermediateSplitWall|LeafSplitCpu|LeafSplitScheduled|LeafSplitWait|LeafSplitWall)Time.P(\d+)'
          valueFactor: 0.000001
          name: 'trino_task_executor_$1_seconds'
          help: '$1 in seconds for the labelled quantile'
          type: GAUGE
          labels:
            quantile: '0.$2'
        - pattern: 'trino.execution.executor<name=TaskExecutor><>(IntermediateSplitCpu|IntermediateSplitScheduled|IntermediateSplitWait|IntermediateSplitWall|LeafSplitCpu|LeafSplitScheduled|LeafSplitWait|LeafSplitWall)Time.Count'
          valueFactor: 0.000001
          name: 'trino_task_executor_$1_seconds_count'
          help: '$1 in seconds count'
          type: COUNTER
        # }}}
        # }}}

        # Trino resource group stats {{{
        - pattern: 'trino.execution.resourcegroups<type=InternalResourceGroup, name=(.+)><>(HardConcurrencyLimit|MaxQueuedQueries|QueuedQueries|RunningQueries|WaitingQueuedQueries)'
          name: 'trino_resourcegroups_$2'
          help: '$2 for the labelled resource group name'
          type: GAUGE
          labels:
            group: '$1'
        # }}}

        # Trino cluster size {{{
        - pattern: 'trino.failuredetector<name=HeartbeatFailureDetector><>(Active|Failed|Total)Count'
          name: 'trino_failuredetector_$1_nodes'
          help: 'Number of $1 nodes observed by the failure detector'
          type: GAUGE
        # }}}

        # Trino memory stats {{{
        - pattern: 'trino.memory<name=ClusterMemoryManager><>Cluster(MemoryBytes|TotalMemoryReservation|UserMemoryReservation)'
          name: 'trino_cluster_memory_manager_$1_bytes'
          help: 'Cluster $1 in bytes'
          type: GAUGE
        - pattern: 'trino.memory<name=ClusterMemoryManager><>TotalAvailableProcessors'
          name: 'trino_cluster_memory_manager_available_processors'
          help: 'Cluster total available CPU cores'
          type: GAUGE
        - pattern: 'trino.memory<name=ClusterMemoryManager><>(NumberOfLeakedQueries|QueriesKilledDueToOutOfMemory)'
          name: 'trino_cluster_memory_manager_$1_total'
          help: 'Total $1'
          type: COUNTER

        # Cluster wide memory pools
        - pattern: 'trino.memory<type=ClusterMemoryPool, name=(.+)><>(AssignedQueries|BlockedNodes|Nodes)'
          name: 'trino_cluster_memory_pool_$2'
          help: '$1 in the labelled cluster memory pool'
          type: GAUGE
          labels:
            pool: '$1'
        - pattern: 'trino.memory<type=ClusterMemoryPool, name=(.+)><>(Free|Reserved|ReservedRevocable|Total)DistributedBytes'
          name: 'trino_cluster_memory_pool_$2_distributed_bytes'
          help: '$1 distributed memory in the labelled cluster memory pool in bytes'
          type: GAUGE
          labels:
            pool: '$1'

        # Local memory
        - pattern: 'trino.memory<type=MemoryPool, name=(.+)><>(Free|Max|Reserved|ReservedRevocable)Bytes'
          name: 'trino_memory_pool_$2_bytes'
          help: '$1 memory in the labelled memory pool in bytes'
          type: GAUGE
          labels:
            pool: '$1'
        # }}}

        # Trino metadata {{{
        - pattern: 'trino.metadata<name=DiscoveryNodeManager><>(Active|Inactive|ShuttingDown)NodeCount'
          name: 'trino_metadata_$1_nodes'
          help: 'Number of $1 nodes as seen by discovery service'
          type: GAUGE
        # }}}
        # Trino security stats {{{
        - pattern: 'trino.security<name=AccessControlManager><>(.+).TotalCount'
          name: 'trino_security_$1_count_total'
          help: 'Number of $1'
          type: COUNTER
        # }}}

        # SQL generator stats {{{
        - pattern: 'trino.sql.gen<name=(.+)><>(.+).(Hit|Miss)Rate'
          name: 'trino_sql_gen_$1_$2_$3_rate'
          type: GAUGE
        - pattern: 'trino.sql.gen<name=(.+)><>(.+).RequestCount'
          name: 'trino_sql_gen_$1_$2_request_count'
          type: COUNTER
        # }}}

        # SQL Planner stats {{{
        - pattern: 'trino.sql.planner.(.+)<name=(.+), rule=(.+)><>(Failures|Hits)'
          name: 'trino_sql_planner_$2_$4'
          type: COUNTER
          labels:
            rule: '$3'

        - pattern: 'trino.sql.planner.(.+)<name=(.+), rule=(.+)><>Time.(Avg|Min|Max)'
          valueFactor: 0.000001
          name: 'trino_sql_planner_$2_seconds_$4'
          type: GAUGE
          labels:
            rule: '$3'
        - pattern: 'trino.sql.planner.(.+)<name=(.+), rule=(.+)><>Time.P(\d+)'
          valueFactor: 0.000001
          name: 'trino_sql_planner_$2_seconds'
          type: GAUGE
          labels:
            rule: '$3'
            quantile: '0.$4'
        - pattern: 'trino.sql.planner.(.+)<name=(.+), rule=(.+)><>Time.Count'
          name: 'trino_sql_planner_$2_seconds_count'
          type: COUNTER
          labels:
            rule: '$3'
            # }}}
      startDelaySeconds: 30
fullnameOverride: alluxio

s3:
  bucket: s3://{bucket_name}
  endpoint:
    host:
    region:
  accessKey:
  secretKey:



## Common ##

# Docker Image
alluxio: 
  image: alluxio/alluxio
  imageTag: 2.7.2
  imagePullPolicy: IfNotPresent

# Security Context
user: 1000
group: 1000
fsGroup: 1000

# Service Account
#   If not specified, Kubernetes will assign the 'default'
#   ServiceAccount used for the namespace
serviceAccount:

imagePullSecrets:

# Site properties for all the components
properties:
  # alluxio.user.metrics.collection.enabled: 'true'
  alluxio.security.stale.channel.purge.interval: 365d
  alluxio.hub.manager.rpc.hostname: alluxio-hub


## Master ##

master:
  enabled: true
  hostname: alluxio-master-0
  count: 1 # Controls the number of StatefulSets. For multiMaster mode increase this to >1.
  replicas: 1 # Controls #replicas in a StatefulSet and should not be modified in the usual case.
  env:
    # Extra environment variables for the master pod
    # Example:
    # JAVA_HOME: /opt/java
  args: # Arguments to Docker entrypoint
    - master-only
    - --no-format
  config:
    path: /conf
    subPath: alluxio-site.properties
  # Properties for the master component
  properties:
    # Example: use ROCKS DB instead of Heap
    # alluxio.master.metastore: ROCKS
    # alluxio.master.metastore.dir: /metastore

  resources:
    # The default xmx is 8G
    limits:
      cpu: "4"
      memory: "8G"
    requests:
      cpu: "1"
      memory: "1G"
  ports:
    rpc: 19998
    web: 19999
    embedded: 19200
  hostPID: false
  hostNetwork: false
  
  jvmOptions:
  nodeSelector: {}
  
  readinessProbe:
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    failureThreshold: 3
    successThreshold: 1
  livenessProbe:
    initialDelaySeconds: 15
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 2
  
  tolerations: []
  podAnnotations: {}
  # The ServiceAccount provided here will have precedence over
  # the global `serviceAccount`
  serviceAccount:

jobMaster:
  args:
    - job-master
  # Properties for the jobMaster component
  properties:
  resources:
    limits:
      cpu: "4"
      memory: "8G"
    requests:
      cpu: "1"
      memory: "1G"
  ports:
    rpc: 20001
    web: 20002
    embedded: 20003
  # JVM options specific to the jobMaster container
  jvmOptions:
  readinessProbe:
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    failureThreshold: 3
    successThreshold: 1
  livenessProbe:
    initialDelaySeconds: 15
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 2
  
journal:
  type: "UFS" # "UFS" or "EMBEDDED"
  ufsType: "local" # Ignored if type is "EMBEDDED". "local" or "HDFS"
  folder: "/journal" # Master journal folder
  # volumeType controls the type of journal volume.
  # It can be "persistentVolumeClaim" or "emptyDir"
  volumeType: persistentVolumeClaim
  size: 1Gi
  # Attributes to use when the journal is persistentVolumeClaim
  storageClass: "standard"
  accessModes:
    - ReadWriteOnce
  # Attributes to use when the journal is emptyDir
  medium: ""
  # Configuration for journal formatting job
  format:
    runFormat: false # Change to true to format journal


## Worker ##

worker:
  enabled: true
  hostname: alluxio-worker-0
  count: 1
  user: 0
  env:
    # Extra environment variables for the worker pod
    # Example:
    # JAVA_HOME: /opt/java
  args:
    - worker-only
    - --no-format
  # Properties for the worker component
  properties:
  resources:
    limits:
      cpu: "4"
      memory: "4G"
    requests:
      cpu: "1"
      memory: "2G"
  ports:
    rpc: 29999
    web: 30000
  # hostPID requires escalated privileges
  hostPID: false
  hostNetwork: false
  # dnsPolicy will be ClusterFirstWithHostNet if hostNetwork: true
  # and ClusterFirst if hostNetwork: false
  # You can specify dnsPolicy here to override this inference
  # dnsPolicy: ClusterFirst
  # JVM options specific to the worker container
  jvmOptions:
  nodeSelector: {}
  readinessProbe:
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    failureThreshold: 3
    successThreshold: 1
  livenessProbe:
    initialDelaySeconds: 15
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 2
  # If you are using Kubernetes 1.18+ or have the feature gate
  # for it enabled, use startupProbe to prevent the livenessProbe
  # from running until the startupProbe has succeeded
  # startupProbe:
  #   initialDelaySeconds: 15
  #   periodSeconds: 30
  #   timeoutSeconds: 5
  #   failureThreshold: 2
  tolerations: []
  podAnnotations: {}
  # The ServiceAccount provided here will have precedence over
  # the global `serviceAccount`
  serviceAccount:
  # Setting fuseEnabled to true will embed Fuse in worker process. The worker pods will
  # launch the Alluxio workers using privileged containers with `SYS_ADMIN` capability.
  # Be sure to give root access to the pod by setting the global user/group/fsGroup
  # values to `0` to turn on Fuse in worker.
  fuseEnabled: false

jobWorker:
  args:
    - job-worker
  # Properties for the jobWorker component
  properties:
  resources:
    limits:
      cpu: "4"
      memory: "4G"
    requests:
      cpu: "1"
      memory: "1G"
  ports:
    rpc: 30001
    data: 30002
    web: 30003
  # JVM options specific to the jobWorker container
  jvmOptions:
  readinessProbe:
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    failureThreshold: 3
    successThreshold: 1
  livenessProbe:
    initialDelaySeconds: 15
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 2
  
tieredstore:
  levels:
  - level: 0
    alias: MEM
    mediumtype: MEM
    path: /dev/shm
    type: emptyDir
    quota: 1G
    high: 0.95
    low: 0.7

# Short circuit related properties
shortCircuit:
  enabled: true
  # The policy for short circuit can be "local" or "uuid",
  # local means the cache directory is in the same mount namespace,
  # uuid means interact with domain socket
  policy: uuid
  # volumeType controls the type of shortCircuit volume.
  # It can be "persistentVolumeClaim" or "hostPath"
  volumeType: persistentVolumeClaim
  size: 1Mi
  # Attributes to use if the domain socket volume is PVC
  pvcName: alluxio-worker-domain-socket
  accessModes:
    - ReadWriteOnce
  storageClass: standard
  # Attributes to use if the domain socket volume is hostPath
  hostPath: "/tmp/alluxio-domain" # The hostPath directory to use


##  Metrics System ##

# Settings for Alluxio metrics. Disabled by default.
metrics:
  enabled: false
  # Enable ConsoleSink by class name
  ConsoleSink:
    enabled: false
    # Polling period for ConsoleSink
    period: 10
    # Unit of poll period
    unit: seconds
  # Enable CsvSink by class name
  CsvSink:
    enabled: false
    # Polling period for CsvSink
    period: 1
    # Unit of poll period
    unit: seconds
    # Polling directory for CsvSink, ensure this directory exists!
    directory: /tmp/alluxio-metrics
  # Enable JmxSink by class name
  JmxSink:
    enabled: false
    # Jmx domain
    domain: org.alluxio
  # Enable GraphiteSink by class name
  GraphiteSink:
    enabled: false
    # Hostname of Graphite server
    host: NONE
    # Port of Graphite server
    port: NONE
    # Poll period
    period: 10
    # Unit of poll period
    unit: seconds
    # Prefix to prepend to metric name
    prefix: ""
  # Enable Slf4jSink by class name
  Slf4jSink:
    enabled: false
    # Poll period
    period: 10
    # Units of poll period
    unit: seconds
    # Contains all metrics
    filterClass: null
    # Contains all metrics
    filterRegex: null
  # Enable PrometheusMetricsServlet by class name
  PrometheusMetricsServlet:
    enabled: false
  # Pod annotations for Prometheus
  # podAnnotations:
  #   prometheus.io/scrape: "true"
  #   prometheus.io/port: "19999"
  #   prometheus.io/path: "/metrics/prometheus/"
  podAnnotations: {}
